{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "import re\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "from attention import Attention_layer\n",
    "\n",
    "#import spacy\n",
    "#from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.layers import Input,GRU, Dense, Dropout,Conv1D, Conv2D, LSTM,Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from bs4 import BeautifulSoup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tweet=pd.read_csv('tweet_lemmatization.csv')\n",
    "mbti=tweet['type']\n",
    "tweet['E/I']=tweet['type'].apply(lambda s: s[0])\n",
    "tweet['N/S']=tweet['type'].apply(lambda s: s[1])\n",
    "tweet['F/T']=tweet['type'].apply(lambda s: s[2])\n",
    "tweet['J/P']=tweet['type'].apply(lambda s: s[3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(tweet,mbti,test_size=0.1,random_state=42)\n",
    "\n",
    "X_train=X_train.reset_index(drop=True)\n",
    "y_train=y_train.reset_index(drop=True)\n",
    "X_test=X_test.reset_index(drop=True)\n",
    "y_test=y_test.reset_index(drop=True)\n",
    "\n",
    "y_train_df=pd.DataFrame()\n",
    "y_train_df['E/I']=y_train.apply(lambda x: int(x[0]=='E'))\n",
    "y_train_df['S/N']=y_train.apply(lambda x: int(x[1]=='S'))\n",
    "y_train_df['F/T']=y_train.apply(lambda x: int(x[2]=='F'))\n",
    "y_train_df['P/J']=y_train.apply(lambda x: int(x[3]=='P'))\n",
    "y_train_1=y_train_df['E/I'].values.reshape(-1,1)\n",
    "y_train_2=y_train_df['S/N'].values.reshape(-1,1)\n",
    "y_train_3=y_train_df['F/T'].values.reshape(-1,1)\n",
    "y_train_4=y_train_df['P/J'].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "y_test_df=pd.DataFrame()\n",
    "y_test_df['E/I']=y_test.apply(lambda x: int(x[0]=='E'))\n",
    "y_test_df['S/N']=y_test.apply(lambda x: int(x[1]=='S'))\n",
    "y_test_df['F/T']=y_test.apply(lambda x: int(x[2]=='F'))\n",
    "y_test_df['P/J']=y_test.apply(lambda x: int(x[3]=='P'))\n",
    "y_test_1=y_test_df['E/I'].values.reshape(-1,1)\n",
    "y_test_2=y_test_df['S/N'].values.reshape(-1,1)\n",
    "y_test_3=y_test_df['F/T'].values.reshape(-1,1)\n",
    "y_test_4=y_test_df['P/J'].values.reshape(-1,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load('tweet_w2v.model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (7807, 1000, 300)\n",
      "Shape of feature matrix: (868, 1000, 300)\n"
     ]
    }
   ],
   "source": [
    "def vectorize_reviews(data, maxlen=1000, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    Tokenizes reviews, truncates the number of tokens if more than `maxlen`,\n",
    "    and vectorizes each token. Returns a three-dimensional array of shape\n",
    "    n reviews x `maxlen` x `embedding_dim`.\n",
    "    \"\"\"\n",
    "    # Create empty array\n",
    "    vectorized_data = np.zeros(shape=(len(data), maxlen, embedding_dim))\n",
    "\n",
    "    for row, review in enumerate(data):\n",
    "        # Preprocess each review\n",
    "        tokens = simple_preprocess(review)\n",
    "\n",
    "        # Truncate long reviews\n",
    "        if len(tokens) > maxlen:\n",
    "            tokens = tokens[:maxlen]\n",
    "\n",
    "        # Get vector for each token in review\n",
    "        for col, token in enumerate(tokens):\n",
    "            try:\n",
    "                word_vector = w2v.wv[token]\n",
    "                # Add vector to array\n",
    "                vectorized_data[row, col] = word_vector[:embedding_dim]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    return vectorized_data\n",
    "\n",
    "maxlen = 1000       # Our predetermined limit\n",
    "embedding_dim = 300 # The first 100 values in our w2v vectors\n",
    "\n",
    "X_train = vectorize_reviews(X_train.posts, maxlen, embedding_dim)\n",
    "X_test= vectorize_reviews(X_test.posts, maxlen, embedding_dim)\n",
    "\n",
    "\n",
    "print('Shape of feature matrix:', X_train.shape)\n",
    "print('Shape of feature matrix:', X_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m attention_model3\u001B[38;5;241m=\u001B[39m \u001B[43mSequential\u001B[49m([\n\u001B[1;32m      2\u001B[0m     Input(shape\u001B[38;5;241m=\u001B[39m(maxlen, embedding_dim),name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m      3\u001B[0m     LSTM(\u001B[38;5;241m64\u001B[39m,return_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlstm\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m      4\u001B[0m     Attention_layer(units\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m,name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m      5\u001B[0m     Dense(\u001B[38;5;241m16\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m'\u001B[39m,name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdense\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m      6\u001B[0m     Dense(\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m'\u001B[39m,name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m ])\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# print(attention_model.summary())\u001B[39;00m\n\u001B[1;32m     11\u001B[0m attention_model3\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     12\u001B[0m                   metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     13\u001B[0m                   optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "attention_model3= Sequential([\n",
    "    Input(shape=(maxlen, embedding_dim),name='input'),\n",
    "    LSTM(64,return_sequences=True,name='lstm'),\n",
    "    Attention_layer(units=128,name='attention'),\n",
    "    Dense(16, activation='sigmoid',name='dense'),\n",
    "    Dense(1, activation='sigmoid',name='output')\n",
    "])\n",
    "\n",
    "# print(attention_model.summary())\n",
    "\n",
    "attention_model3.compile(loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  optimizer='adam')\n",
    "\n",
    "history = attention_model3.fit(X_train, y_train_3, epochs=8,\n",
    "                              validation_split=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'model/model3.pkl'\n",
    "pickle.dump(attention_model3, open(filename, 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = pickle.load(open('model.pkl','rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}